{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用朴素贝叶斯完成语种检测\n",
    "拉丁语系，字母差不多，根据字母使用的顺序和频度分类，粒度是字母，而不是词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:22:54.214853Z",
     "start_time": "2019-07-30T14:22:54.191852Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../nlp/input/data.csv',header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到这个数据没有属性头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:23:39.850759Z",
     "start_time": "2019-07-30T14:23:39.819158Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme</th>\n",
       "      <th>nl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 millón de afectados ante las inundaciones en...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 millón de fans en facebook antes del 14 de f...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 satellite galileo sottoposto ai test presso ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 der welt sind bei</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 jaar voor overval op juwelier bejaard echtp...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme  \\\n",
       "0  1 millón de afectados ante las inundaciones en...                                      \n",
       "1  1 millón de fans en facebook antes del 14 de f...                                      \n",
       "2  1 satellite galileo sottoposto ai test presso ...                                      \n",
       "3                               10 der welt sind bei                                      \n",
       "4  10 jaar voor overval op juwelier bejaard echtp...                                      \n",
       "\n",
       "   nl  \n",
       "0  es  \n",
       "1  es  \n",
       "2  it  \n",
       "3  de  \n",
       "4  nl  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:37:20.278502Z",
     "start_time": "2019-07-30T14:37:20.229301Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_f = open('../nlp/input/data.csv',encoding='utf-8')\n",
    "lines = in_f.readlines()\n",
    "in_f.close()\n",
    "\n",
    "#strip()移除字符串头尾的指定字符，默认是空格\n",
    "#[ :倒数第三个]截取\n",
    "#将line切分成两块，整个是作为list的\n",
    "dataset = [(line.strip()[:-3],line.strip()[-2:]) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:37:22.733013Z",
     "start_time": "2019-07-30T14:37:22.712413Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme',\n",
       "  'nl'),\n",
       " ('1 millón de afectados ante las inundaciones en sri lanka unicef está distribuyendo ayuda de emergencia srilanka',\n",
       "  'es'),\n",
       " ('1 millón de fans en facebook antes del 14 de febrero y paty miki dani y berta se tiran en paracaídas qué harías tú porunmillondefans',\n",
       "  'es'),\n",
       " ('1 satellite galileo sottoposto ai test presso lesaestec nl galileo navigation space in inglese',\n",
       "  'it'),\n",
       " ('10 der welt sind bei', 'de')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]#一个样本是一个二元组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 做训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:45:27.699923Z",
     "start_time": "2019-07-30T14:45:27.628721Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#zip(压缩，两个分别取一个元素打包成元组，组成一个新list) zip(*)将原来是元组的解压成原来的\n",
    "#二元组展开成两个\n",
    "x,y = zip(*dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:47:16.319050Z",
     "start_time": "2019-07-30T14:47:16.294449Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme',\n",
       " '1 millón de afectados ante las inundaciones en sri lanka unicef está distribuyendo ayuda de emergencia srilanka',\n",
       " '1 millón de fans en facebook antes del 14 de febrero y paty miki dani y berta se tiran en paracaídas qué harías tú porunmillondefans',\n",
       " '1 satellite galileo sottoposto ai test presso lesaestec nl galileo navigation space in inglese',\n",
       " '10 der welt sind bei')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:48:13.225548Z",
     "start_time": "2019-07-30T14:48:13.211547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:48:47.301808Z",
     "start_time": "2019-07-30T14:48:47.297808Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6799"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理\n",
    "由于原文是从类似微博拉取的数据，所以需要去除一些噪声，如：@，#等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T15:14:19.589000Z",
     "start_time": "2019-07-30T15:14:19.570999Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trump images are now more popular than cat gifs.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#我们用正则表达式，去掉噪声数据\n",
    "def remove_noise(document):\n",
    "    noise_pattern = re.compile(\"|\".join([\"http\\S+\", \"\\@\\w+\", \"\\#\\w+\"]))\n",
    "    clean_text = re.sub(noise_pattern, \"\", document)\n",
    "    return clean_text.strip()\n",
    "\n",
    "remove_noise(\"Trump images are now more popular than cat gifs. @trump #trends http://www.trumptrends.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽取n-gram统计特征 向量化\n",
    "给每个出现的最小粒度的元素，都给了一个编码\n",
    "\n",
    "(123 24 56 1024 1567)\n",
    "\n",
    "trump images are now... => 1gram = t,r,u,m,p... 2gram = tr,ru,um,mp...<br>\n",
    "先用全部的词语fit，得到1或者2gram的高频词和编号<br>\n",
    "transform每个样本的词，得到在相应词的编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T15:26:04.429133Z",
     "start_time": "2019-07-30T15:26:03.199287Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "model = CountVectorizer(\n",
    "    lowercase=True,    # 英文文本全小写\n",
    "    analyzer='char_wb', # 逐个字母解析\n",
    "    ngram_range=(1,2), # 1=出现的字母以及每个字母出现的次数，2=出现的连续2个字母，和连续2个字母出现的频次 使用1-gram和2gram\n",
    "    max_features =1000,#选1000个高频，提升效果可以增加词表数量也可以加n-gram或者改模型\n",
    "    preprocessor=remove_noise\n",
    ")\n",
    "model.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    model.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T15:32:33.722036Z",
     "start_time": "2019-07-30T15:32:33.715035Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t18\n",
      "  (0, 11)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 150)\t5\n",
      "  (0, 159)\t1\n",
      "  (0, 166)\t1\n",
      "  (0, 170)\t2\n",
      "  (0, 172)\t1\n",
      "  (0, 212)\t1\n",
      "  (0, 215)\t1\n",
      "  (0, 272)\t3\n",
      "  (0, 273)\t2\n",
      "  (0, 293)\t1\n",
      "  (0, 302)\t1\n",
      "  (0, 321)\t1\n",
      "  (0, 330)\t2\n",
      "  (0, 338)\t1\n",
      "  (0, 342)\t1\n",
      "  (0, 359)\t1\n",
      "  :\t:\n",
      "  (0, 536)\t2\n",
      "  (0, 537)\t1\n",
      "  (0, 553)\t1\n",
      "  (0, 570)\t3\n",
      "  (0, 590)\t1\n",
      "  (0, 592)\t1\n",
      "  (0, 597)\t1\n",
      "  (0, 603)\t3\n",
      "  (0, 604)\t1\n",
      "  (0, 619)\t1\n",
      "  (0, 624)\t1\n",
      "  (0, 644)\t4\n",
      "  (0, 645)\t1\n",
      "  (0, 651)\t2\n",
      "  (0, 667)\t1\n",
      "  (0, 682)\t2\n",
      "  (0, 683)\t2\n",
      "  (0, 717)\t2\n",
      "  (0, 718)\t1\n",
      "  (0, 726)\t1\n",
      "  (0, 754)\t2\n",
      "  (0, 767)\t1\n",
      "  (0, 768)\t1\n",
      "  (0, 809)\t1\n",
      "  (0, 810)\t1\n"
     ]
    }
   ],
   "source": [
    "result = model.transform([\"Trump images are now more popular than cat gifs\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备ML模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T15:37:06.511646Z",
     "start_time": "2019-07-30T15:37:05.201386Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf =MultinomialNB()\n",
    "clf.fit(model.transform(x_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T15:38:05.551473Z",
     "start_time": "2019-07-30T15:38:05.111250Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9770621967357741"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(model.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 规范化，写成一个class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T15:38:42.518046Z",
     "start_time": "2019-07-30T15:38:42.460443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "class LanguageDetector():\n",
    "\n",
    "    def __init__(self, classifier=MultinomialNB()):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer(ngram_range=(1,2), max_features=1000, preprocessor=self._remove_noise)\n",
    "\n",
    "    def _remove_noise(self, document):\n",
    "        noise_pattern = re.compile(\"|\".join([\"http\\S+\", \"\\@\\w+\", \"\\#\\w+\"]))\n",
    "        clean_text = re.sub(noise_pattern, \"\", document)\n",
    "        return clean_text\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T15:39:23.571038Z",
     "start_time": "2019-07-30T15:39:22.426597Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en']\n",
      "0.9770621967357741\n"
     ]
    }
   ],
   "source": [
    "in_f = open('../nlp/input/data.csv',encoding='utf-8')\n",
    "lines = in_f.readlines()\n",
    "in_f.close()\n",
    "dataset = [(line.strip()[:-3], line.strip()[-2:]) for line in lines]\n",
    "x, y = zip(*dataset)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n",
    "\n",
    "language_detector = LanguageDetector()\n",
    "language_detector.fit(x_train, y_train)\n",
    "print(language_detector.predict('This is an English sentence'))\n",
    "print(language_detector.score(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda]",
   "language": "python",
   "name": "conda-env-Anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
